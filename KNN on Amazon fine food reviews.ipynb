{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN on Amazon Fine Food Reviews dataset\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n",
    "\n",
    "Number of reviews: 568,454\n",
    "Number of users: 256,059\n",
    "Number of products: 74,258\n",
    "Timespan: Oct 1999 - Oct 2012\n",
    "Number of Attributes/Columns in data: 10\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review\n",
    "\n",
    "#### Objective:\n",
    "Classifying Amazon fine food reviews with polarity based color-coding (that means predicting wheater review is positive or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The dataset is available in two forms\n",
    "1. .csv file\n",
    "2. SQLite Database\n",
    "\n",
    "In order to load the data, We have used the SQLITE dataset as it easier to query the data and visualise the data efficiently.\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "con = sqlite3.connect('database.sqlite') #connecting to 'database.sqlite' and reading data from it \n",
    "\n",
    "filter_amz_data = pd.read_sql_query(\"\"\" \n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3\n",
    "\"\"\", con) # taking only those reviews having score greater than 3(4,5) and less than 3(1,2) not taking reviews of score 3\n",
    "\n",
    "\n",
    "def partition_positive_negative(x): # Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filter_amz_data['Score']\n",
    "positiveNegative = actualScore.map(partition_positive_negative) #map() will maps inputs of 'actualscore' using function partition\n",
    "filter_amz_data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_amz_data = filter_amz_data.head(30000) #Sampled amazon fine foood reviews filtered data to 15k datapoints for time effiecieny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exploratory Data Analysis\n",
    "\n",
    "## [7.1.2] Data Cleaning: Deduplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order; sorting is necessary because if duplicates entries are there than we want to have only one of it and that we get from first entry of duplicates entries after sorting\n",
    "\n",
    "sorted_data = sample_amz_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sorting is necessary because if duplicates entries are there than we want to have only one of it and that we get from first entry of duplicates entries after sorting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 10)\n",
      "(28072, 10)\n"
     ]
    }
   ],
   "source": [
    "#Deduplication of entries\n",
    "\n",
    "final = sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "print(sorted_data.shape) \n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see that in sorted_data there are 15k points but now after removing duplicates entries we git some 14k points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "# It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator \n",
    "#which is not practically possible hence these two rows too are removed from calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28072, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "positive    23606\n",
       "negative     4466\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before starting the next phase of preprocessing lets see the number of entries left\n",
    "print(final.shape)\n",
    "\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text Preprocessing: Stemming, stop-word removal and Lemmatization.\n",
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords # nltk- natural language processing toolkit\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = set(stopwords.words('english')) #set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "# this code takes a while to run as it needs to run on 500k sentences.\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'].values: #taking each reviews\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split(): # taking each words of each reviews\n",
    "        for cleaned_words in cleanpunc(w).split(): #cleanpunc(w) will remove any punctuations/special symbols from each word\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):   #checking word should be only have alphabets not aplha-numeric/numeric and all words should have length >2 \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8') #returing each word to be in lower case\n",
    "                    filtered_sentence.append(s) # appending each filtered word of reviews to the list\n",
    "                    if (final['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sorting dataset based on 'Time' feature</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_Time_data = final.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN using 'Brute force'\n",
    "\n",
    "<b><h1>Techniques for vectorization :--</h1> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(28072, 19423)\n",
      "(19651, 19423)\n",
      "(19651,)\n",
      "(8421, 19423)\n",
      "(8421,)\n",
      "(19651, 19423)\n",
      "(8421, 19423)\n"
     ]
    }
   ],
   "source": [
    "#Bow\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer() \n",
    "final_counts_Bow= count_vect.fit_transform(sorted_Time_data['CleanedText'].values) # computing Bow\n",
    "print(type(final_counts_Bow))\n",
    "print(final_counts_Bow.shape)\n",
    "\n",
    "#Spliting dataset to training(with first 70% points) and test(remaining 30% points) dataset\n",
    "n=28072 \n",
    "tr=final_counts_Bow[0:(int(n*0.7))+1,:] #taking first 70% of points from sorted dataset based on 'Time' feature\n",
    "labels_Bow_tr=sorted_Time_data['Score'].head(int(n*0.7)+1)\n",
    "\n",
    "\n",
    "ts=final_counts_Bow[(int(n*0.7))+1:,:]   #taking remaining 30% of points from sorted dataset based on 'Time' feature\n",
    "labels_Bow_ts=sorted_Time_data['Score'].tail(int(n*0.3))\n",
    "print(tr.shape)\n",
    "print(labels_Bow_tr.shape)\n",
    "print(ts.shape)\n",
    "print(labels_Bow_ts.shape)\n",
    "\n",
    "\n",
    "# Data-preprocessing: Standardizing the data\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "standardized_data_train = MaxAbsScaler().fit_transform(tr)\n",
    "standardized_data_test = MaxAbsScaler().fit_transform(ts)\n",
    "print(standardized_data_train.shape)\n",
    "print(standardized_data_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.stats._distn_infrastructure.rv_frozen"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "a=stats.uniform(2, 10)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of neighbors is 5.\n"
     ]
    }
   ],
   "source": [
    "#Calculating 10 fold CV \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "myList = list(range(0,50))\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,algorithm='brute', n_jobs=-1)\n",
    "    scores = cross_val_score(knn, standardized_data_train, labels_Bow_tr, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the knn classifier for k = 5 is 83.493647%\n",
      "\n",
      "The accuracy of the knn classifier for k = 5 is 86.972673%\n"
     ]
    }
   ],
   "source": [
    "# ============================== KNN with k = optimal_k ===============================================\n",
    "# instantiate learning model k = optimal_k\n",
    "knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "\n",
    "# fitting the model\n",
    "knn_optimal.fit(standardized_data_train, labels_Bow_tr)\n",
    "\n",
    "# predict the response\n",
    "pred = knn_optimal.predict(standardized_data_test)\n",
    "pred1 = knn_optimal.predict(standardized_data_train)\n",
    "\n",
    "\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(labels_Bow_ts, pred) * 100\n",
    "acc1 = accuracy_score(labels_Bow_tr, pred1) * 100\n",
    "\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc))\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(28072, 19423)\n",
      "(19651, 19423)\n",
      "(19651,)\n",
      "(8421, 19423)\n",
      "(8421,)\n",
      "(19651, 19423)\n",
      "(8421, 19423)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "final_counts_tfidf= tf_idf_vect.fit_transform(sorted_Time_data['CleanedText'].values) \n",
    "print(type(final_counts_tfidf))\n",
    "print(final_counts_tfidf.shape)\n",
    "\n",
    "#Spliting dataset to training(with first 70% points) and test(remaining 30% points) dataset\n",
    "n=len(sorted_Time_data)\n",
    "tr_tfidf=final_counts_tfidf[0:(int(n*0.7))+1,:]  #taking first 70% of points from sorted dataset based on 'Time' feature\n",
    "labels_tfidf_tr=sorted_Time_data['Score'].head(int(n*0.7)+1)\n",
    "\n",
    "ts_tfidf=final_counts_tfidf[(int(n*0.7))+1:,:]   #taking remaining 30% of points from sorted dataset based on 'Time' feature\n",
    "labels_tfidf_ts=sorted_Time_data['Score'].tail(int(n*0.3))\n",
    "print(tr_tfidf.shape)\n",
    "print(labels_tfidf_tr.shape)\n",
    "print(ts_tfidf.shape)\n",
    "print(labels_tfidf_ts.shape)\n",
    "\n",
    "\n",
    "# Data-preprocessing: Standardizing the data\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "standardized_data_train_tfidf = MaxAbsScaler().fit_transform(tr_tfidf)\n",
    "standardized_data_test_tfidf = MaxAbsScaler().fit_transform(ts_tfidf)\n",
    "print(standardized_data_train_tfidf.shape)\n",
    "print(standardized_data_test_tfidf.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of neighbors is 5.\n"
     ]
    }
   ],
   "source": [
    "#Calculating 10 fold CV \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "myList_tfidf = list(range(0,50))\n",
    "neighbors_tfidf = list(filter(lambda x: x % 2 != 0, myList_tfidf))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores_tfidf = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors_tfidf:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,algorithm='brute', n_jobs=-1)\n",
    "    scores = cross_val_score(knn, standardized_data_train_tfidf, labels_tfidf_tr, cv=10, scoring='accuracy')\n",
    "    cv_scores_tfidf.append(scores.mean())\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE_tfidf = [1 - x for x in cv_scores_tfidf]\n",
    "\n",
    "# determining best k\n",
    "optimal_k_tfidf = neighbors_tfidf[MSE_tfidf.index(min(MSE_tfidf))]\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the knn classifier for k = 5 is 82.947393%\n"
     ]
    }
   ],
   "source": [
    "# ============================== KNN with k = optimal_k ===============================================\n",
    "# instantiate learning model k = optimal_k\n",
    "knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k_tfidf)\n",
    "\n",
    "# fitting the model\n",
    "knn_optimal.fit(standardized_data_train_tfidf, labels_tfidf_tr)\n",
    "\n",
    "# predict the response\n",
    "pred_tfidf = knn_optimal.predict(standardized_data_test_tfidf)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_tfidf = accuracy_score(labels_tfidf_ts, pred_tfidf) * 100\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k_tfidf, acc_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Average word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our own Word2Vec model using our own text corpus\n",
    "import gensim\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in sorted_Time_data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):    \n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue \n",
    "    list_of_sent.append(filtered_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=1,size=50, workers=4)    \n",
    "# min_count here says that if a word doesn't occur atleast 5 times than not construct its word2vector\n",
    "# size means dimension of each vector of word  , here is 50 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19651\n",
      "(19651,)\n",
      "8421\n",
      "(8421,)\n",
      "(19651, 50)\n",
      "(8421, 50)\n"
     ]
    }
   ],
   "source": [
    "#Spliting dataset to training(with first 70% points) and test(remaining 30% points) dataset\n",
    "\n",
    "n=len(sent_vectors)\n",
    "tr_avgw2v=sent_vectors[0:(int(n*0.7))+1]  #taking first 70% of points from sorted dataset based on 'Time' feature\n",
    "labels_avgw2v_tr=sorted_Time_data['Score'].head(int(n*0.7)+1)\n",
    "\n",
    "ts_avgw2v=sent_vectors[(int(n*0.7))+1:]  #taking remaining 30% of points from sorted dataset based on 'Time' feature\n",
    "labels_avgw2v_ts=sorted_Time_data['Score'].tail(int(n*0.3))\n",
    "print(len(tr_avgw2v))\n",
    "print(labels_avgw2v_tr.shape)\n",
    "print(len(ts_avgw2v))\n",
    "print(labels_avgw2v_ts.shape)\n",
    "\n",
    "# Data-preprocessing: Standardizing the data\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "standardized_data_train_avgw2v = MaxAbsScaler().fit_transform(tr_avgw2v)\n",
    "standardized_data_test_avgw2v = MaxAbsScaler().fit_transform(ts_avgw2v)\n",
    "print(standardized_data_train_avgw2v.shape)\n",
    "print(standardized_data_test_avgw2v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of neighbors is 17.\n"
     ]
    }
   ],
   "source": [
    "#Calculating 10 fold CV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "myList_avgw2v = list(range(0,50))\n",
    "neighbors_avgw2v = list(filter(lambda x: x % 2 != 0, myList_avgw2v))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores_avgw2v = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors_avgw2v:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,algorithm='brute', n_jobs=-1)\n",
    "    scores = cross_val_score(knn, standardized_data_train_avgw2v, labels_avgw2v_tr, cv=10, scoring='accuracy')\n",
    "    cv_scores_avgw2v.append(scores.mean())\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE_avgw2v = [1 - x for x in cv_scores_avgw2v]\n",
    "\n",
    "# determining best k\n",
    "optimal_k_avgw2v = neighbors_avgw2v[MSE_avgw2v.index(min(MSE_avgw2v))]\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k_avgw2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the knn classifier for k = 17 is 84.799905%\n"
     ]
    }
   ],
   "source": [
    "# ============================== KNN with k = optimal_k ===============================================\n",
    "# instantiate learning model k = optimal_k\n",
    "knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k_avgw2v)\n",
    "\n",
    "# fitting the model\n",
    "knn_optimal.fit(standardized_data_train_avgw2v, labels_avgw2v_tr)\n",
    "\n",
    "# predict the response\n",
    "pred_avgw2v = knn_optimal.predict(standardized_data_test_avgw2v)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_avgw2v = accuracy_score(labels_avgw2v_ts, pred_avgw2v) * 100\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k_avgw2v, acc_avgw2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) TF-IDF Weighted Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(28072, 31375)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf weighted w2v\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfw2v_vect = TfidfVectorizer()\n",
    "final_counts_tfidfw2v= tfidfw2v_vect.fit_transform(sorted_Time_data['Text'].values) \n",
    "print(type(final_counts_tfidfw2v))\n",
    "print(final_counts_tfidfw2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=tfidfw2v_vect.get_feature_names()\n",
    "t1=list()\n",
    "ro=0\n",
    "for sen in list_of_sent:\n",
    "    revec=np.zeros(50)\n",
    "    wrsu=0\n",
    "    for wro in sen:\n",
    "        try:\n",
    "            wve=w2v_model.wv[wro]\n",
    "            tfv=final_counts_tfidfw2v[ro,t.index(wro)]\n",
    "            revec += (wve*tfv)\n",
    "            wrsu += tfv\n",
    "        except:\n",
    "            pass\n",
    "    revec /= wrsu\n",
    "    t1.append(revec)\n",
    "    ro += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19651\n",
      "(19651,)\n",
      "8421\n",
      "(8421,)\n",
      "(19651, 50)\n",
      "(8421, 50)\n"
     ]
    }
   ],
   "source": [
    "#Spliting dataset to training(with first 70% points) and test(remaining 30% points) dataset\n",
    "\n",
    "n=len(t1)\n",
    "tr_tfidfw2v=t1[0:(int(n*0.7))+1]  #taking first 70% of points from sorted dataset based on 'Time' feature\n",
    "labels_tfidfw2v_tr=sorted_Time_data['Score'].head(int(n*0.7)+1)\n",
    "\n",
    "ts_tfidfw2v=t1[(int(n*0.7))+1:]  #taking remaining 30% of points from sorted dataset based on 'Time' feature\n",
    "labels_tfidfw2v_ts=sorted_Time_data['Score'].tail(int(n*0.3))\n",
    "print(len(tr_tfidfw2v))\n",
    "print(labels_tfidfw2v_tr.shape)\n",
    "print(len(ts_tfidfw2v))\n",
    "print(labels_tfidfw2v_ts.shape)\n",
    "\n",
    "# Data-preprocessing: Standardizing the data\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "standardized_data_train_tfidfw2v = MaxAbsScaler().fit_transform(tr_tfidfw2v)\n",
    "standardized_data_test_tfidfw2v = MaxAbsScaler().fit_transform(ts_tfidfw2v)\n",
    "print(standardized_data_train_tfidfw2v.shape)\n",
    "print(standardized_data_test_tfidfw2v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of neighbors is 15.\n"
     ]
    }
   ],
   "source": [
    "# Calculating 10 fold CV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "myList_tfidfw2v = list(range(0,50))\n",
    "neighbors_tfidfw2v = list(filter(lambda x: x % 2 != 0, myList_tfidfw2v))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores_tfidfw2v = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors_tfidfw2v:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,algorithm='brute', n_jobs=-1)\n",
    "    scores = cross_val_score(knn, standardized_data_train_tfidfw2v, labels_tfidfw2v_tr, cv=10, scoring='accuracy')\n",
    "    cv_scores_tfidfw2v.append(scores.mean())\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE_tfidfw2v = [1 - x for x in cv_scores_tfidfw2v]\n",
    "\n",
    "# determining best k\n",
    "optimal_k_tfidfw2v = neighbors_tfidfw2v[MSE_tfidfw2v.index(min(MSE_tfidfw2v))]\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k_tfidfw2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the knn classifier for k = 15 is 84.170526%\n"
     ]
    }
   ],
   "source": [
    "# ============================== KNN with k = optimal_k ===============================================\n",
    "# instantiate learning model k = optimal_k\n",
    "knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k_tfidfw2v)\n",
    "\n",
    "# fitting the model\n",
    "knn_optimal.fit(standardized_data_train_tfidfw2v, labels_tfidfw2v_tr)\n",
    "\n",
    "# predict the response\n",
    "pred_tfidfw2v = knn_optimal.predict(standardized_data_test_tfidfw2v)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_tfidfw2v = accuracy_score(labels_tfidfw2v_ts, pred_tfidfw2v) * 100\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k_tfidfw2v, acc_tfidfw2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Final Observation(s):--</b>\n",
    "\n",
    "1. After all text-preprocesing step we have sorted the final dataset based on 'Time' features.\n",
    "\n",
    "2. Than for each featurization or vectorization techniques we have first split the dataset into Training dataset(first 70% of points) than test dataset(remaining 30% of points).\n",
    "\n",
    "3. Optimal 'K' and accuracy score for each techniques using 'Brute force' method are as follows:--\n",
    "\n",
    "    For **Bow** the accuracy of the Knn classifier for **K=5** is **83.493647%**\n",
    "    \n",
    "    For **Tf-df** the accuracy of the Knn classifier for **K=5** is **82.947393%**\n",
    "    \n",
    "    For **Avgw2v** the accuracy of the Knn classifier for **K=17** is **84.799905%**\n",
    "    \n",
    "    For **Tfidfw2v** the accuracy of the Knn classifier for **K=15** is **84.170526%**\n",
    "\n",
    "4. And found that Average word2vec perform better than other techniques.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
